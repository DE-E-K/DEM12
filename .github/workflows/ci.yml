name: CI — Build & Test

on:
  push:
    branches: ["**"]
  pull_request:
    branches: [main]
  workflow_dispatch:


jobs:
  # === 0. Prepare shared CI environment =====================================
  prepare-env:
    name: Prepare CI .env
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Copy env template
        run: cp .env.example .env

      - name: Set valid Airflow Fernet key for CI
        run: |
          FERNET_KEY=$(python3 - <<'PY'
          import base64, os
          print(base64.urlsafe_b64encode(os.urandom(32)).decode())
          PY
          )
          sed -i "s|^AIRFLOW__CORE__FERNET_KEY=.*|AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}|" .env

      - name: Upload CI .env artifact
        uses: actions/upload-artifact@v4
        with:
          name: ci-env-file
          path: .env
          retention-days: 1

  # === 1. Validate Compose file =============================================─
  lint-compose:
    name: Lint docker-compose.yml
    runs-on: ubuntu-latest
    needs: prepare-env
    steps:
      - uses: actions/checkout@v4

      - name: Download CI .env artifact
        uses: actions/download-artifact@v4
        with:
          name: ci-env-file
          path: .

      - name: Validate Compose config
        run: docker compose config --quiet

  # === 2. Build all images ======================================================
  build:
    name: Build Docker images
    runs-on: ubuntu-latest
    needs: [prepare-env, lint-compose]
    steps:
      - uses: actions/checkout@v4

      - name: Download CI .env artifact
        uses: actions/download-artifact@v4
        with:
          name: ci-env-file
          path: .

      - name: Build images
        run: docker compose build --no-cache data-generator

      - name: Save data-generator image
        run: docker save data-platform/data-generator:latest -o /tmp/data-generator.tar

      - name: Cache image artifact
        uses: actions/upload-artifact@v4
        with:
          name: data-generator-image
          path: /tmp/data-generator.tar
          retention-days: 1

  # === 3. Unit tests ===============================================================
  unit-tests:
    name: Unit Tests (transformations + db_loader)
    runs-on: ubuntu-latest
    needs: [prepare-env, build]
    steps:
      - uses: actions/checkout@v4

      - name: Download CI .env artifact
        uses: actions/download-artifact@v4
        with:
          name: ci-env-file
          path: .

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install test dependencies
        run: |
          pip install pandas pyarrow psycopg2-binary pydantic pydantic-settings pytest pytest-cov boto3

      - name: Run unit tests
        run: pytest tests/test_data_flow.py::TestTransformationUnit -v --tb=short
        env:
          PYTHONPATH: .
          POSTGRES_HOST: localhost
          POSTGRES_PORT: "5432"
          POSTGRES_DB: sales
          POSTGRES_USER: sales_user
          POSTGRES_PASSWORD: test_password_ci
          AIRFLOW_ADMIN_USERNAME: admin
          MINIO_ROOT_USER: minioadmin
          MINIO_ROOT_PASSWORD: minioadmin123
          MINIO_ENDPOINT: http://localhost:9000
          MINIO_RAW_BUCKET: raw-data
          MINIO_PROCESSED_BUCKET: processed-data
          AIRFLOW_ADMIN_PASSWORD: admin_password_ci
          AIRFLOW_ADMIN_EMAIL: admin@example.com
          GENERATOR_MIN_ROWS: "50"
          GENERATOR_MAX_ROWS: "50"
          GENERATOR_SEED: "42"

  # === 4. DAG import check ======================================================
  dag-integrity:
    name: Airflow DAG import check
    runs-on: ubuntu-latest
    needs: [prepare-env, lint-compose]
    steps:
      - uses: actions/checkout@v4

      - name: Download CI .env artifact
        uses: actions/download-artifact@v4
        with:
          name: ci-env-file
          path: .

      # === Start postgres and wait for the health check to pass ===
      - name: Start PostgreSQL
        run: |
          docker compose up -d postgres
          echo "Waiting for postgres to be healthy (pg_isready)..."
          timeout 60 bash -c \
            'until docker inspect platform_postgres \
              --format "{{.State.Health.Status}}" 2>/dev/null | grep -q healthy; \
            do sleep 3; done'
          echo "postgres is healthy."

      # === pg_isready passes BEFORE init-scripts finish.
      #    Poll until the 'airflow' DB exists, which proves 01_schema.sql ran. ===
      - name: Wait for PostgreSQL init-scripts to finish
        run: |
          echo "Waiting for init-scripts to create the 'airflow' database..."
          timeout 90 bash -c \
            'until docker exec platform_postgres \
              psql -U sales_user -d sales -tAc \
              "SELECT 1 FROM pg_database WHERE datname='"'"'airflow'"'"'" \
              2>/dev/null | grep -q 1; \
            do sleep 3; done'
          echo "Init-scripts complete — 'airflow' database confirmed."

      # === Run airflow-init in FOREGROUND (--no-deps: postgres is already up).
      #    set -e inside the script means any failure = non-zero exit = CI fails. ===
      - name: Initialise Airflow databases
        run: docker compose run --rm --no-deps airflow-init

      # === Run the DAG list check without starting the full webserver ===
      - name: Test DAG import
        run: |
          docker compose run --rm --no-deps airflow-scheduler \
            airflow dags list 2>&1 | tee /tmp/dag-list.txt
          grep -q "sales_pipeline_dag" /tmp/dag-list.txt \
            && echo "DAG found." \
            || { echo "sales_pipeline_dag not found in dag list"; exit 1; }

      - name: Tear down
        if: always()
        run: docker compose down -v


